# 基础概念
- 偏导数(partial derivative)
- 梯度(gradient),记作$\nabla_xf(\boldsymbol{x})$.
- 方向导数(directional derivative):f在u方向的斜率，$\frac{\partial}{\partial\alpha}f(\boldsymbol x+\alpha\boldsymbol u)=\boldsymbol u^{\top}\nabla_{x}f(\boldsymbol x)$.就是该方向乘以梯度
- 一阶优化算法(first-order optimization algorithms):仅使用一阶导数(梯度)信息
- 二阶优化算法(second-order optimization algorithms):使用Hessian矩阵的优化算法
- 凸优化(Convex optimization)

# Jacobian矩阵
多元函数(自变量，因变量均为多元)中每一个因变量对全部自变量导数组成的矩阵。$\boldsymbol{f}\colon\mathbb{R}^m\to\mathbb{R}^n,\boldsymbol{f}$的Jacobian矩阵 $J\in\mathbb{R}^{n\times m}$定义为$J_{i,j}=\frac{\partial}{\partial x_j}f(\boldsymbol{x})_{i}$.

# Hessian矩阵
[Hessian矩阵和极值判断 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/377754969).
黑塞(Hessian)矩阵定义为多元函数二阶导数的矩阵，$$\boldsymbol H(f)(\boldsymbol x)_{i,j}=\dfrac{\partial^2}{\partial x_i\partial x_j}f(\boldsymbol x)$$Hessian等价于梯度的Jacobian矩阵:$\boldsymbol H(f)(\boldsymbol x)_{i,j}=\nabla_{x}(\nabla_{x}f(x))^{T}$.
## Hessian矩阵的特征分解
Hessian矩阵是实对称矩阵，可以分解为一组标准正交特征向量和与其对应的特征值。
$$H=\operatorname{Ddiag}(\Lambda) D^{T} \\
=\left[d_{1}, d_{2}, \ldots, d_{n}\right]\left[\begin{array}{cccc}
\lambda_{1} & & & \\
& \lambda_{2} & & \\
& & \cdots & \\
& & & \lambda_{n}
\end{array}\right]\left[\begin{array}{c}
d_{1}^{T} \\
d_{2}^{T} \\
\cdots \\
d_{n}^{T}
\end{array}\right] \\
=\sum_{i}^{n} d_{i} \lambda_{i} d_{i}^{T}$$函数沿某一方向的二阶方向导数可以表示为$d^THd$.当d是H的特征向量时，对应的特征值便是二阶方向导数;当
d指向其他方向是，d可以由H的特征向量组线性表出，方向二阶导数是所有特征值的加权平均:
$$\begin{array}{l}
d^{T} H d=\left(a_{1} d_{1}^{T} H+a_{2} d_{2}^{T} H+\ldots+a_{n} d_{n}^{T} H\right)\left(a_{1} d_{1}+a_{2} d_{2}+\ldots+a_{n} d_{n}\right) \\
=a_{1}^{2} d_{1}^{T} H d_{1}+a_{2}^{2} d_{2}^{T} H d_{2}+\ldots+a_{n}^{2} d_{n}^{T} H d_{n} \\
=a_{1}^{2} \lambda_{1}+a_{2}^{2} \lambda_{2}+\ldots+a_{n}^{2} \lambda_{n}
\end{array}$$
## Hessian矩阵判断函数极值
- 正定(所有特征值>0):局部极小点
- 负定(所有特征值<0):局部极大点
- 至少有一个特征值大于零，且至少一个特征值小于零，那么在为大于零的特征值对应的横截面上是局部极小值，在小于零的特征值对应的横截面身上是局部极大值，为鞍点
- 其他特征值都为正(或负)，但是至少有一个特征值为0;即Hessian矩阵半正定(或半负定),无法判断。
## 牛顿法
通过Hessian矩阵迭代自变量，寻找函数最小值。利用Hessian矩阵实现优化算法。用二阶泰勒展开式近似f(x),对该展开式求导并令导数为0，得到:$$x^*=x^{(0)}-H(f)(x^{(0)})^{-1}\nabla_xf(\boldsymbol{x^{(0)}}).$$在接近局部极小点时非常有用，在鞍点附近则是有害的
# KKT方法
[Karush-Kuhn-Tucker (KKT)条件 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/38163970).
和考研数学中学到的拉格朗日乘数法类似，将约束优化转化为非约束优化，求导，令导数为零并算出符合约束条件的驻点。
优化目标:
$$\begin{array}{ll}\min&f(\mathbf{x})\\ \text{s.t.}&g_j(\mathbf{x})=0,\quad j=1,\dots,m,\\ &h_k(\mathbf{x})\leq0,\quad k=1,\dots,p.\end{array}$$
定义Lagrangian 函数:
$$L\left(\mathbf{x},\left\{\lambda_{j}\right\},\left\{\mu_{k}\right\}\right)=f(\mathbf{x})+\sum_{j=1}^{m}\lambda_{j}g_{j}(\mathbf x)+\sum_{k=1}^{p}\mu_{k}h_{k}(\mathbf x)\quad$$
KKT条件:
$$\begin{matrix}\nabla_{\mathbf{x}}L=\mathbf{0}\\ g_{j}(\mathbf{x})=0,j=1,\ldots,m,\\ h_{k}(\mathbf{x})\le0,\\ \mu_{k}\ge0,\\ \mu_{k}h_{k}({\mathbf{x}})=0,k=1,\ldots,p.\end{matrix}$$
# 线性最小二乘
优化目标是$f(x)=\dfrac{1}{2}\|Ax-b\|_2^2$.可以从梯度下降和矩阵计算两方面入手解决
## 梯度下降
- 求函数梯度$\nabla_xf(\boldsymbol{x})=\boldsymbol{A}^{\top}(\boldsymbol{Ax}-\boldsymbol{b})={\boldsymbol{A}}^{\top}\boldsymbol{Ax}-{\boldsymbol{A}^\top}\boldsymbol{b}$.
- 在梯度值大于阈值时进行迭代，$x\leftarrow x-\epsilon\left(A^\top A x-A^\top b\right)$.
## 矩阵计算
直接令梯度=0，计算x:${\boldsymbol{A}}^{\top}\boldsymbol{Ax}-{\boldsymbol{A}^\top}\boldsymbol{b}=0$.$x=A^Tb(A^TA)^{-1}$.
## 两种方法对比
- 最小二乘法需要计算$A^TA$的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了;此时梯度下降法仍然可以使用
- 当样本特征n非常的大的时候，计算$A^TA$的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），甚至不可行，只得通过主成分分析降低特征的维度后再用最小二乘法。此时以梯度下降为代表的迭代法仍然可以使用。