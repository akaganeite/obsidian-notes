# 1. 基本概念
- 标量(scalar)，一个单独的数
- 向量(vector)，有序排列的一列数，如果向量中每个元素都属于$R$且向量有n个元素，则向量表示为$R^n$
	- **$\color{Orchid}{向量被视作空间中的点，每个元素是不同坐标轴的坐标}$**
- 矩阵(matrix)，是一个二维数组，用$A\in R^n$表示
	- $f(A)_{i,j}$表示函数f作用在A矩阵上输出的矩阵的ij元素
- 张量(tensor)，多维数组
- 点积(dot product)，表示两个向量x，y以$x^T y$的形式进行矩阵乘法运算，即对应元素乘积之和
	- **$\color{Orchid}{x^T y=y^T x}$，满足交换律
	- 几何意义：$a \cdot b=|a| \times|b| \times \cos (\theta)$,可以用于计算余弦相似度
- 范数(norm)：将向量映射到非负值的函数，定义为$\|x\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{\frac{1}{P}}$,称为p-范数
	- p=2，欧几里得范数，向量距原点距离，$|x\|_{2}=|x\|=x^{T}x$.
	- p=1,向量每个元素绝对值之和
	- p=∞，$\|x\|_{\infty}=\max _{i}\left|x_{i}\right|$.
	- Frobenius范数：衡量矩阵大小，$\|\boldsymbol{A}\|_{F}=\sqrt{\sum_{i, j} A_{i, j}^{2}}$.
- 正交(orthogonal):若向量x，y正交，那么$x^T y=0$.也就是二维平面中的垂直
	- 标准正交(orthonormal):向量不仅正交，且范数均为1，即单位向量
	- $\color{Orchid}{正交矩阵(orthogonal matrix)}$:行向量与列向量分别是**标准正交**的，$A^{-1}=A^T,A^TA=AA^T=I$.
- 迹运算(trace):$\operatorname{Tr}(\boldsymbol{A})=\sum_{i} \boldsymbol{A}_{i, i}$


# 2.线性方程组
## 2.1线性方程组的概念
$Ax=b$.其中$A\in R^{mxn},\ b\in R^m,\ x\in R^n$.分别代表已知矩阵，已知向量和未知向量，上述表达可以写作：  
$$
\begin{array}{c}
\boldsymbol{A}_{1,1} x_{1}+\boldsymbol{A}_{1,2} x_{2}+\cdots \boldsymbol{A}_{1, n} x_{n}=b_{1} \\
\boldsymbol{A}_{2,1} x_{1}+\boldsymbol{A}_{2,2} x_{2}+\cdots \boldsymbol{A}_{2, n} x_{n}=b_{2} \\
\cdots \\
\boldsymbol{A}_{m, 1} x_{1}+\boldsymbol{A}_{m, 2} x_{2}+\cdots \boldsymbol{A}_{m, n} x_{n}=b_{m} .
\end{array}
$$
## 2.2线性方程组求解
这里略过通过高斯消元求通解，特解这样的具体技巧，这些技巧可以被用于求解拥有数千个参数的方程组。 
***
在特殊情况下，矩阵A存在逆矩阵，那么$x=A^{-1}b$.
但大部分时候矩阵A并不可逆，这时可以使用**Moore-Penrose_伪逆**，
$$Ax=b\Longleftrightarrow A^\top A x=A^\top b\Longleftrightarrow x=\left(A^\top A\right)^{-1}A^\top b$$
这里的矩阵A需要有线性独立的列向量组，且A的列数大于行数，这个解法对应于最小范数最小二乘法。 
***
在实际应用中使用的是**定常迭代方法(_stationary iterative methods_**)间接求解的(行大于列，没有解析解)，其关键思想是建立$x^{(k+1)}=Cx^{(k)}+d$.寻找适当的C和d，使得$||x^{(k+1)}-x^{\star }||$.逐渐减小，最终收敛至$x^{\star}$.

### 对于求解的另一个理解方式
向量b视作空间中的一个点，A的列向量看作从远点出发的不同方向，给每个方向前乘一个系数，表示沿该方向前进的距离，这个系数组成的向量即为x.用数学公式表达为
$$\sum_i x_i\mathbf{A}_{:,i}.=\mathbf{Ax}=b$$
最左边的和操作被称为**线性组合(linear combination)**.
一组向量的**生成子空间(span)**,是原始向量线性组合后能抵达的点的集合.**向量空间(vector space)** 的定义涉及群的知识，不在此讨论.对于$Ax=b$.当且仅当A的列空间包含b所在的那个点时方程有解，而A的列空间可以描述的维度又是由A的列向量极大无关组决定的。因此，当方程有解时，A的列数一定大于等于行数，即n≥m。
>如果一个矩阵的列空间涵盖整个$R^m$,那么该矩阵必须含有至少一组m个线性无关的向量


# 3.特征分解
特征分解只针对方阵，它将一个方阵分解为一组特征向量和一组特征值
$$Av=\lambda v$$
A为方阵，v是一个特征向量，λ是该向量对应的特征值。每个实对称矩阵都可以分解为实特征向量和实特征值，即$\boldsymbol{A}=\boldsymbol{Q \Lambda} \boldsymbol{Q}^{\top}$.Q为正交矩阵，$\boldsymbol{\Lambda}$为对角矩阵，主对角线元素代表不同特征值。幸运的是PCA中特征分解的对象是一个协方差矩阵，是实对称的,可以直接分解为上述形式

# 4.SVD分解
SVD分解(singular value decomposition)将矩阵分解为三个矩阵的乘积形式
$$A=UDV^T,A\in R^{m*n},U\in R^{m*m},D\in R^{m*n},V\in R^{n*n}$$
D被定义为对角矩阵；U，V均为正交矩阵

# 5.主成分分析
[CodingLabs - PCA的数学原理](http://blog.codinglabs.org/articles/pca-tutorial.html)
主成分分析(principal components analysis)被用于对数据进行有损压缩(降维)。对$R^n$空间中的m个点{$X^{(1)},...,X^{(m)}$}.对每个点$X^{(i)}$(n维)用一个更低维度的编码$c^{(i)}$($l$维).编码间的映射通过矩阵乘法实现，$$f(x)=c,x\approx g(f(x));f(x)=D^Tx,r(x)=g(f(x))=DD^Tx$$
其中，$x\in R^{n*m},c\in R^{l*m},D^T\in R^{l*n}$,PCA限制D的所有列向量彼此正交，这一限制是为了简化编码。
## 求解最优D矩阵
求解D矩阵等价于最小化输入和重构之间的欧几里得范数。对于矩阵来说，是最小化输入矩阵X与重构矩阵$DD^TX$的Frobenius范数:
$$D^*=\arg\min\sqrt{\sum\limits_{i,j}\left(\boldsymbol{x}_j^{(i)}-r(\boldsymbol{x}^{(i)})_j\right)^2}\text{subject to}D^{\top}D=I_l$$
化简为:
$$\underset{d}{\operatorname{argmax}}\operatorname{Tr}(d^\top X^\top X\boldsymbol d)\text{subject to}d^\top\boldsymbol d=1，l=1$$
最大化迹，就是求$\boldsymbol{\Lambda}=d^\top X^\top X\boldsymbol d$,$\boldsymbol{\Lambda}$为最大特征向量,d为最大特征值对应的特征向量。
推广至一般情况：
- 对$\frac{1}{m}X^TX$进行特征分解
- 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵$D^T$
- $C=D^TX$即为降维到k维后的数据
