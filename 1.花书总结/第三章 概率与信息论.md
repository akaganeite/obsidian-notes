# 1.基础概念
## 1.1概念，定义，法则
- 离散型变量:**概率质量函数(probability mass function,PMF)**，也被称为分布律
	- 多个变量的概率分布:**联合概率分布(joint probability distribution)**
- 连续型变量:**概率密度函数(probability density function,**$\color{Orchid}{PDF}$)
- 边缘概率:**边缘概率分布(marginal probability distribution)**
	- 离散:$\forall x\in\mathrm x,P(\mathrm x=x)=\sum\limits_{y}P(\mathrm x=x,\mathrm y=y)$.
	- 连续:$p(x)=\int p(x,y)dy$.
- 条件概率:$P(\mathrm y=y\mid\mathrm x=x)=\dfrac{P(\mathrm y=y,\mathrm x=x)}{P(\mathrm x=x)}$.
- ***条件概率的链式法则***:任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相乘的形式$$P(\mathrm{x}^{(1)},\ldots,\mathrm{x}^{(n)})=P(\mathrm x^{(1)})\Pi_{i\equiv2}^n P(\mathrm x^{(i)}\mid\mathrm x^{(1)},\cdots,\mathrm x^{(i-1)}).$$                                             $\color{violet}{非常重要的法则}$
## 1.2期望，方差，协方差
### 1.2.1 期望
函数f(x)关于某分布P(x)的**期望(expectation)**，是$\color{Orchid}{指当x由P产生,f作用于x时,f(x)的平均值}$.
$$
\begin{array} \\ 
 离散:\ \ \mathbb{E}_{x\sim P}[f(x)]=\sum P(x)f(x) \\ 
 连续:\ \ \mathbb{E}_{\text{x}\sim p}[f(x)]=\int p(x)f(x)dx
\end{array}
$$若P已指明，则简化为$\mathbb{E}_{\text{x}}[f(x)]$,我们假设$\mathbb{E}[\cdot]$表示对方括号内的所有随机变量的值求平均.
### 1.2.2 方差
方差(variance)表示对x依据它的概率分布进行采样时，随机变量x的函数值会呈现多大的差异:$\operatorname{Var}(f(x))=\mathbb{E}\left[(f(x)-\mathbb{E}[f(x)])^2\right]$.
协方差(covariance)衡量两个变量线性相关性的强度:$\operatorname{Cov}(f(x),g(y))=\mathbb E[(f(x)-\mathbb E[f(x)])(g(y)-\mathbb E[g(y)])]$.
协方差矩阵(covariance matrix):
$$\Sigma=\left[\begin{array}{ccc}
\sigma\left(x_{1}, x_{1}\right) & \cdots & \sigma\left(x_{1}, x_{d}\right) \\
\vdots & \ddots & \vdots \\
\sigma\left(x_{d}, x_{1}\right) & \cdots & \sigma\left(x_{d}, x_{d}\right)
\end{array}\right] \in \mathbb{R}^{d \times d}$$对角线上的元素为各个随机变量的方差，非对角线上的元素为两两随机变量之间的协方差:$$\mathrm{Cov}(\mathbf{x})_{i,j}=\mathrm{Cov}({\mathrm{x}_i},{\mathrm{x}_j}),\operatorname{Cov}(\mathrm x_i,\mathrm x_i)=\operatorname{Var}(\mathrm x_i)$$根据协方差的定义，我们可以认定：矩阵 Σ 为**对称矩阵**(symmetric matrix)，其大小为dxd.协方差矩阵可以进行特征值分解.在多元正态分布中,多元正态分布的概率密度是由**协方差矩阵的特征向量控制旋转(rotation)**，**特征值控制尺度(scale)**，除了协方差矩阵，**均值向量会控制概率密度的位置**.
# 2.常用概率分布
## 2.1 Bernoulli分布
也即二项分布,是单个二值随机变量的分布,由单个参数控制
## 2.2Multinouli分布
是多项式分布的一个特例,可以理解为二项分布的拓展.
## 2.3高斯分布
高斯分布(Gaussian distribution),又名正态分布(normal distribution).概率密度为:
$$\mathcal{N}(x;\mu,\sigma^2)=\sqrt{\dfrac{1}{2\pi\sigma^2}}\exp\left(-\dfrac{1}{2\sigma^2}(x-\mu)^2\right)$$推广至多维:
$${\mathcal{N}}(\boldsymbol{x};\boldsymbol{\mu},\boldsymbol{\Sigma})={\sqrt{\frac{1}{(2\pi)^{n}\operatorname*{det}(\boldsymbol{\Sigma})}}}\exp\left(-{\frac{1}{2}}(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{(x}-\mu)\right)$$$\mu$为分布均值,是一个向量,$\boldsymbol{\Sigma}$是分布的协方差矩阵.常把协方差矩阵固定为对角阵,也即不同变量之间的协方差均为0
## 2.4 分布的混合
混合分布由一些组件分布构成,组件之间服从Multinouli分布:$P(\mathrm x)=\sum_i P(\mathrm c=i)P(\mathrm x\mid\mathrm c=i)$.P(c)是组件所服从的分布。
**潜变量(latent variable)**:不能直接观测的随机变量，如上文提到的c。举例:高斯混合模型，组件为高斯分布。
# 3信息论
自信息(self-information):$I(x)=-logP(x)$.
香农熵(Shannon entropy):$H(\mathrm x)=\mathbb E_{x\sim P}[I(x)]=-\mathbb E_{\mathrm x\sim P}[\log P(x)]$.对自信息取期望
KL散度(KL divergence):$D_{\operatorname{KL}}(P||Q)=\operatorname{E}_{x\in P}\left[\log\dfrac{P(x)}{Q(x)}\right]=\mathbb{E}_{x\sim P}[\log P(x)-\log Q(x)]$.KL散度用于衡量两个分布之间的差异，但KL散度是不对称的，因此不可以直接等同于两个分布之间的“距离”。
KL散度的直观解释是:在离散变量的情况下，使用针对概率Q的最小编码发送包含概率分布P产生的符号的消息时所需的额外信息量。
交叉熵(cross-entropy):$H(P,Q)=H(P)+D_{KL}(P||Q)=-\mathbb{E}_{\mathbb{x}P}\log Q(x)$.